{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b6336b-fd63-4251-b037-4b0c2f0240cc",
   "metadata": {},
   "source": [
    "# Test of first, basic model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff9429d-ec72-4e35-bfa5-e815befcd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 23:46:27.151970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-04 23:46:27.152026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-04 23:46:27.153546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-04 23:46:27.162088: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 23:46:28.084636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-04 23:46:29.824122: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-04 23:46:29.852907: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-04 23:46:29.853248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "# Set logging level to avoid unnecessary messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "# Set autograph verbosity to avoid unnecessary messages\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import keras\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_extra as tfe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "XC_ROOTDIR = '../../data/' # directory to save data in\n",
    "XC_DIR = 'test_dataset10' # subdirectory name of dataset\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a9cad4-87aa-4b6f-871a-ebda0a2b1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069df00-2fba-496d-b715-1e8bf1ebe043",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f6d1e2-6699-4983-9638-5d38b3395d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13875"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/dataset_train.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582e4e1-cb13-4714-93a9-4ade01a2debf",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ce8f59-d368-48d4-88fb-ed0d0733e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    # random seed\n",
    "    seed = 42\n",
    "\n",
    "    # audio clip settings\n",
    "    sr = 22050\n",
    "    duration = 15 # the duration of the clips\n",
    "    \n",
    "    n_samples = duration*sr\n",
    "    \n",
    "    hop_length = 2048 # \"stepsize\" of the fft for the melspectrograms\n",
    "    nfft = 4096 # windowsize of the fft for the melspectrograms\n",
    "    n_mels = 128 # number of mel frequency bins\n",
    "    fmax = sr/2 # maximum frequency in the melspectrograms\n",
    "    input_dim = (n_mels, int(duration*sr//hop_length + 1))\n",
    "    \n",
    "    # training settings\n",
    "    batch_size = 32\n",
    "    n_epochs = 50\n",
    "    \n",
    "    # class labels/names\n",
    "    names = list(np.unique(df.en))\n",
    "    n_classes = len(names)\n",
    "    labels = list(range(n_classes))\n",
    "    label2name = dict(zip(labels, names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "\n",
    "# set random seed in keras\n",
    "tf.keras.utils.set_random_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486464b-e522-4b07-9648-31d58c19cb4f",
   "metadata": {},
   "source": [
    "### Up- and downsample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c2acf5-0cf2-4dcb-b561-3d45ec90158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_data(df, thr=200):\n",
    "    # get the class distribution\n",
    "    class_dist = df['en'].value_counts()\n",
    "\n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    down_classes = class_dist[class_dist < thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    up_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in down_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"en==@c\")\n",
    "        # find number of samples to add\n",
    "        num_up = thr - class_df.shape[0]\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=num_up, replace=True, random_state=cfg.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        up_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    up_df = pd.concat([df] + up_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return up_df\n",
    "\n",
    "def downsample_data(df, thr=400):\n",
    "    # get the class distribution\n",
    "    class_dist = df['en'].value_counts()\n",
    "    \n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    up_classes = class_dist[class_dist > thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    down_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in up_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"en==@c\")\n",
    "        # Remove that class data\n",
    "        df = df.query(\"en!=@c\")\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=thr, replace=False, random_state=cfg.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        down_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    down_df = pd.concat([df] + down_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return down_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f572a6-89a2-45e0-b941-df7e837dde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = upsample_data(downsample_data(df, thr=100), thr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d378ec-683e-4a70-b50a-accd4ea25262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bda254f-12d2-4abb-bba3-7f9d32065fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4600/4600 [00:00<00:00, 5930.79it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = None\n",
    "\n",
    "for idx in tqdm(df.index):\n",
    "    df.loc[idx, \"label\"] = cfg.name2label[df.loc[idx, \"en\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e088cf8-77f7-4857-959a-a80aae78955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fullfilename = \"../\" + df.fullfilename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29ee37-304f-4b0e-8e0e-aefedc7d38c9",
   "metadata": {},
   "source": [
    "### Functions, data generator etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a1b30c-5e3d-4b96-9f9b-23839921df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates random integer # from https://www.kaggle.com/code/wengsilu/birdclef24pretraining\n",
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "# Generats random float\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd\n",
    "\n",
    "def load_spectrogram_slice(hdf5_path, name, start_row = 0, end_row =None, start_col = 0, end_col = None):\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        spectrogram_slice = f[name][start_row:end_row, start_col:end_col]\n",
    "    return spectrogram_slice\n",
    "\n",
    "def load_random_spec_slice(df, ID):\n",
    "    name = df.spectrogram.iloc[ID]\n",
    "    hdf5_path = os.path.dirname(df.fullfilename.iloc[ID]) + \"/spectrograms.h5\"\n",
    "    spec_length = df.length_spectrogram.iloc[ID]\n",
    "    if spec_length > cfg.input_dim[1]:\n",
    "        rdm = random_int(maxval= spec_length - cfg.input_dim[1])\n",
    "        return load_spectrogram_slice(hdf5_path = hdf5_path, name = name, start_col = rdm, end_col = rdm + cfg.input_dim[1])\n",
    "    else:\n",
    "        return load_spectrogram_slice(hdf5_path = hdf5_path, name = name)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, dataframe,\n",
    "                 batch_size=cfg.batch_size, \n",
    "                 dim=cfg.input_dim,\n",
    "                 n_channels =  1,\n",
    "                 n_classes=cfg.n_classes, \n",
    "                 shuffle=True\n",
    "                ):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dataframe = dataframe\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = load_random_spec_slice(self.dataframe, ID).reshape(*self.dim, self.n_channels)\n",
    "            # Store class\n",
    "            y[i] = cfg.name2label[df.en.iloc[ID]]\n",
    "        X = X.reshape(len(X), *self.dim, self.n_channels)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['accuracy'])\n",
    "    plt.plot(network_history.history['val_accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def pad_spectrogram(spec, shape = cfg.input_dim, random = False):\n",
    "    _ = np.zeros(shape)\n",
    "    if random:\n",
    "        rdm = random_int(maxval=shape[1]-spec.shape[1])\n",
    "        _[:,rdm: rdm + spec.shape[1]] = spec \n",
    "    else:\n",
    "        _[:,:spec.shape[1]] = spec\n",
    "    return _\n",
    "\n",
    "def predict_file(df, ID, model):\n",
    "    name = df.spectrogram.iloc[ID]\n",
    "    hdf5_path = os.path.dirname(df.fullfilename.iloc[ID]) + \"/spectrograms.h5\"\n",
    "    spec_length = df.length_spectrogram.iloc[ID]\n",
    "    spec = load_spectrogram_slice(hdf5_path, name)\n",
    "    slices = []\n",
    "    \n",
    "    for i in range(spec_length//cfg.input_dim[1]):\n",
    "        slices.append(spec[:,i*cfg.input_dim[1]:(i+1)*cfg.input_dim[1]])\n",
    "    if spec_length%cfg.input_dim[1]/cfg.input_dim[1] > 5/cfg.duration:\n",
    "        # consider last slice, only if it is longer than the shortest clips in the dataset \n",
    "        slices.append(pad_spectrogram(spec[:, (i+1)*cfg.input_dim[1]:None], random = True))\n",
    "    \n",
    "    preds = model.predict(np.expand_dims(np.array(slices), axis = -1))\n",
    "    \n",
    "    return np.mean(preds, axis = 0) # return mean prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa05f3-cb66-4340-a2e6-48bb03c32839",
   "metadata": {},
   "source": [
    "## Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e1c8db-8682-4b21-bb07-5f1f29539e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcfr\u001b[0m (\u001b[33mML_Project2024TUD\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/Studium/Seminare/MachineLearningSeminar/CNN-based-Bird-Audio-Classification/workflow/notebooks/wandb/run-20240704_234633-0fd1rrja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification/runs/0fd1rrja' target=\"_blank\">smart-butterfly-1</a></strong> to <a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification' target=\"_blank\">https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification/runs/0fd1rrja' target=\"_blank\">https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification/runs/0fd1rrja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CNN_Birdcall_classification\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"batch_size\": cfg.batch_size,\n",
    "        \"epochs\": cfg.n_epochs,\n",
    "        \"n_classes\": cfg.n_classes,\n",
    "        \"audio duration\": cfg.duration\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cb0c81-50a6-4c41-be67-80df89cf9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
     ]
    }
   ],
   "source": [
    "#wandb_metric = WandbMetricsLogger()\n",
    "\n",
    "path = \"../models/Basemodel_test0.h5\"\n",
    "\n",
    "model_checkpoint = WandbModelCheckpoint(path, \n",
    "                                        monitor = \"val_accuracy\", \n",
    "                                        verbose = 1, \n",
    "                                        save_best_only=True,\n",
    "                                        mode = \"max\"\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1827c3d9-879a-4c00-b08e-7e3327a9d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, CSVLogger #, ModelCheckpoint\n",
    "#path = \"../models/Basemodel_test0.h5\"\n",
    "\n",
    "#checkpoint = ModelCheckpoint(path, \n",
    "#                             monitor = \"val_accuracy\", \n",
    "#                             verbose = 1, \n",
    "#                             save_best_only=True,\n",
    "#                             mode = \"max\"\n",
    "#                            )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=6,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=10,\n",
    ")\n",
    "\n",
    "log_file = \"../models/Basemodel_test0_log.csv\"\n",
    "\n",
    "csv_log = CSVLogger(log_file, separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b64452a-b40a-48f3-9982-cfc062823b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [model_checkpoint, csv_log, WandbMetricsLogger()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a890af-b9ae-449c-aded-07cd71f96937",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af85c771-e64c-4341-82d0-c66b02c87608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "zscore_layer = tfe.layers.ZScoreMinMax()\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape=(*cfg.input_dim, 1))\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(inp)\n",
    "    \n",
    "    # Base model\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(cfg.n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output, name = \"Basemodel\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2563cf-96cf-4b02-a594-798395861a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Basemodel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 162, 1)]     0         \n",
      "                                                                 \n",
      " z_score_min_max (ZScoreMin  (None, 128, 162, 1)       0         \n",
      " Max)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 126, 160, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 80, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 78, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 39, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 39, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 74880)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4792384   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 46)                1518      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4814798 (18.37 MB)\n",
      "Trainable params: 4814798 (18.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8300339-f4bf-4de7-b13b-651bcd2c0247",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e383534-3dcf-49d9-989f-3e7cbbc84048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "id_train, id_val, y_train, y_val = train_test_split(range(len(df)), df[\"en\"].to_list(), test_size = 0.3, random_state = cfg.seed)\n",
    "\n",
    "training_generator = DataGenerator(id_train, y_train, df)\n",
    "validation_generator = DataGenerator(id_val, y_val, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3627f1d-0fb3-41fc-b1f4-db5a6d145893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 23:46:41.964609: I external/local_xla/xla/service/service.cc:168] XLA service 0x5611e046b5d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-04 23:46:41.964652: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-07-04 23:46:41.977486: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720129602.040735    2606 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-07-04 23:46:42.042858: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-07-04 23:46:42.045201: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-07-04 23:46:42.471359: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.01817, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 69s - loss: 3.8326 - accuracy: 0.0181 - val_loss: 3.8290 - val_accuracy: 0.0182 - 69s/epoch - 692ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.01817 to 0.02544, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 66s - loss: 3.8295 - accuracy: 0.0237 - val_loss: 3.8286 - val_accuracy: 0.0254 - 66s/epoch - 656ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.02544\n",
      "100/100 - 70s - loss: 3.8261 - accuracy: 0.0291 - val_loss: 3.8231 - val_accuracy: 0.0174 - 70s/epoch - 700ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.02544\n",
      "100/100 - 64s - loss: 3.8215 - accuracy: 0.0294 - val_loss: 3.8210 - val_accuracy: 0.0240 - 64s/epoch - 640ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.02544 to 0.07267, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 72s - loss: 3.7464 - accuracy: 0.0472 - val_loss: 3.5419 - val_accuracy: 0.0727 - 72s/epoch - 717ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.07267 to 0.16134, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 72s - loss: 3.3239 - accuracy: 0.1075 - val_loss: 3.0680 - val_accuracy: 0.1613 - 72s/epoch - 722ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.16134 to 0.21512, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 65s - loss: 2.9223 - accuracy: 0.1853 - val_loss: 2.7767 - val_accuracy: 0.2151 - 65s/epoch - 653ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.21512 to 0.27035, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 2.6744 - accuracy: 0.2378 - val_loss: 2.5941 - val_accuracy: 0.2703 - 64s/epoch - 637ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.27035 to 0.30451, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 61s - loss: 2.5107 - accuracy: 0.2791 - val_loss: 2.4783 - val_accuracy: 0.3045 - 61s/epoch - 607ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.30451 to 0.32049, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 60s - loss: 2.3959 - accuracy: 0.3022 - val_loss: 2.4498 - val_accuracy: 0.3205 - 60s/epoch - 598ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.32049 to 0.34884, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 69s - loss: 2.3361 - accuracy: 0.3413 - val_loss: 2.3392 - val_accuracy: 0.3488 - 69s/epoch - 686ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.34884 to 0.39680, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 86s - loss: 2.2182 - accuracy: 0.3572 - val_loss: 2.2233 - val_accuracy: 0.3968 - 86s/epoch - 862ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.39680 to 0.40044, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 66s - loss: 2.0942 - accuracy: 0.3941 - val_loss: 2.1528 - val_accuracy: 0.4004 - 66s/epoch - 658ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.40044 to 0.44041, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 61s - loss: 2.0223 - accuracy: 0.4238 - val_loss: 2.0884 - val_accuracy: 0.4404 - 61s/epoch - 614ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.44041 to 0.47166, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.9362 - accuracy: 0.4441 - val_loss: 1.9478 - val_accuracy: 0.4717 - 64s/epoch - 640ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.47166 to 0.47384, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 65s - loss: 1.8526 - accuracy: 0.4663 - val_loss: 1.9188 - val_accuracy: 0.4738 - 65s/epoch - 647ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.47384 to 0.48038, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 65s - loss: 1.8333 - accuracy: 0.4759 - val_loss: 1.8718 - val_accuracy: 0.4804 - 65s/epoch - 648ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.48038 to 0.50436, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 65s - loss: 1.7734 - accuracy: 0.4947 - val_loss: 1.9432 - val_accuracy: 0.5044 - 65s/epoch - 647ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.50436\n",
      "100/100 - 59s - loss: 1.7305 - accuracy: 0.5013 - val_loss: 1.8720 - val_accuracy: 0.4942 - 59s/epoch - 594ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.50436 to 0.51599, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.6877 - accuracy: 0.5069 - val_loss: 1.8371 - val_accuracy: 0.5160 - 64s/epoch - 639ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.51599\n",
      "100/100 - 65s - loss: 1.6544 - accuracy: 0.5275 - val_loss: 1.8077 - val_accuracy: 0.5051 - 65s/epoch - 654ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.51599\n",
      "100/100 - 64s - loss: 1.6139 - accuracy: 0.5297 - val_loss: 1.8047 - val_accuracy: 0.5153 - 64s/epoch - 637ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_accuracy improved from 0.51599 to 0.52762, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.5845 - accuracy: 0.5425 - val_loss: 1.7747 - val_accuracy: 0.5276 - 64s/epoch - 639ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.52762\n",
      "100/100 - 64s - loss: 1.5513 - accuracy: 0.5487 - val_loss: 1.7910 - val_accuracy: 0.5182 - 64s/epoch - 635ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.52762\n",
      "100/100 - 64s - loss: 1.5485 - accuracy: 0.5506 - val_loss: 1.7419 - val_accuracy: 0.5247 - 64s/epoch - 636ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.52762 to 0.55160, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.4831 - accuracy: 0.5697 - val_loss: 1.7102 - val_accuracy: 0.5516 - 64s/epoch - 636ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.55160\n",
      "100/100 - 63s - loss: 1.4442 - accuracy: 0.5888 - val_loss: 1.6585 - val_accuracy: 0.5458 - 63s/epoch - 632ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_accuracy improved from 0.55160 to 0.56831, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 60s - loss: 1.4657 - accuracy: 0.5694 - val_loss: 1.6601 - val_accuracy: 0.5683 - 60s/epoch - 597ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.56831 to 0.57195, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 66s - loss: 1.4403 - accuracy: 0.5716 - val_loss: 1.7235 - val_accuracy: 0.5719 - 66s/epoch - 658ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.57195\n",
      "100/100 - 65s - loss: 1.4078 - accuracy: 0.6031 - val_loss: 1.6892 - val_accuracy: 0.5705 - 65s/epoch - 651ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.57195\n",
      "100/100 - 59s - loss: 1.3983 - accuracy: 0.5925 - val_loss: 1.6990 - val_accuracy: 0.5552 - 59s/epoch - 589ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.57195\n",
      "100/100 - 62s - loss: 1.3812 - accuracy: 0.6037 - val_loss: 1.7435 - val_accuracy: 0.5647 - 62s/epoch - 618ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_accuracy improved from 0.57195 to 0.57267, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 66s - loss: 1.3837 - accuracy: 0.6037 - val_loss: 1.6462 - val_accuracy: 0.5727 - 66s/epoch - 663ms/step\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 00:25:15.446435: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.57267\n",
      "100/100 - 63s - loss: 1.3381 - accuracy: 0.6228 - val_loss: 1.6955 - val_accuracy: 0.5574 - 63s/epoch - 632ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_accuracy improved from 0.57267 to 0.58794, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.3429 - accuracy: 0.6156 - val_loss: 1.6371 - val_accuracy: 0.5879 - 64s/epoch - 637ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.58794\n",
      "100/100 - 64s - loss: 1.3131 - accuracy: 0.6206 - val_loss: 1.6876 - val_accuracy: 0.5705 - 64s/epoch - 644ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_accuracy improved from 0.58794 to 0.59012, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 64s - loss: 1.3297 - accuracy: 0.6266 - val_loss: 1.6209 - val_accuracy: 0.5901 - 64s/epoch - 638ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.59012\n",
      "100/100 - 65s - loss: 1.2885 - accuracy: 0.6319 - val_loss: 1.6671 - val_accuracy: 0.5821 - 65s/epoch - 646ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.59012\n",
      "100/100 - 58s - loss: 1.2813 - accuracy: 0.6234 - val_loss: 1.6579 - val_accuracy: 0.5872 - 58s/epoch - 576ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.59012\n",
      "100/100 - 64s - loss: 1.2697 - accuracy: 0.6328 - val_loss: 1.6785 - val_accuracy: 0.5879 - 64s/epoch - 638ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_accuracy improved from 0.59012 to 0.60465, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 62s - loss: 1.2357 - accuracy: 0.6463 - val_loss: 1.6182 - val_accuracy: 0.6047 - 62s/epoch - 620ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.60465\n",
      "100/100 - 63s - loss: 1.2698 - accuracy: 0.6428 - val_loss: 1.5790 - val_accuracy: 0.5952 - 63s/epoch - 632ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_accuracy improved from 0.60465 to 0.62209, saving model to ../models/Basemodel_test0.h5\n",
      "100/100 - 59s - loss: 1.1878 - accuracy: 0.6544 - val_loss: 1.5723 - val_accuracy: 0.6221 - 59s/epoch - 590ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.62209\n",
      "100/100 - 64s - loss: 1.2494 - accuracy: 0.6356 - val_loss: 1.6323 - val_accuracy: 0.5996 - 64s/epoch - 639ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.62209\n",
      "100/100 - 58s - loss: 1.2272 - accuracy: 0.6359 - val_loss: 1.5773 - val_accuracy: 0.5981 - 58s/epoch - 580ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.62209\n",
      "100/100 - 59s - loss: 1.2008 - accuracy: 0.6562 - val_loss: 1.6007 - val_accuracy: 0.6032 - 59s/epoch - 591ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.62209\n",
      "100/100 - 70s - loss: 1.1915 - accuracy: 0.6572 - val_loss: 1.5133 - val_accuracy: 0.6025 - 70s/epoch - 698ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.62209\n",
      "100/100 - 63s - loss: 1.1864 - accuracy: 0.6472 - val_loss: 1.5901 - val_accuracy: 0.6025 - 63s/epoch - 629ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.62209\n",
      "100/100 - 64s - loss: 1.1641 - accuracy: 0.6522 - val_loss: 1.5648 - val_accuracy: 0.6032 - 64s/epoch - 636ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.62209\n",
      "100/100 - 71s - loss: 1.1892 - accuracy: 0.6534 - val_loss: 1.5509 - val_accuracy: 0.6076 - 71s/epoch - 712ms/step\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(generator=training_generator,\n",
    "                           validation_data=validation_generator,\n",
    "                           use_multiprocessing=True, \n",
    "                           verbose = 2, \n",
    "                           epochs = cfg.n_epochs,\n",
    "                           callbacks = callbacks\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "659ab5ea-6ec6-4e8d-aa5f-d3731096ac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▁▁▁▂▃▃▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>████▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▁▁▁▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>epoch/val_loss</td><td>████▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.65344</td></tr><tr><td>epoch/epoch</td><td>49</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>1.18925</td></tr><tr><td>epoch/val_accuracy</td><td>0.60756</td></tr><tr><td>epoch/val_loss</td><td>1.5509</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-butterfly-1</strong> at: <a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification/runs/0fd1rrja' target=\"_blank\">https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification/runs/0fd1rrja</a><br/> View project at: <a href='https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification' target=\"_blank\">https://wandb.ai/ML_Project2024TUD/CNN_Birdcall_classification</a><br/>Synced 6 W&B file(s), 0 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240704_234633-0fd1rrja/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e5834-d1c1-4cb5-bcf2-f6e9d9e099db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
