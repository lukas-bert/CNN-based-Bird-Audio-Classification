{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b6336b-fd63-4251-b037-4b0c2f0240cc",
   "metadata": {},
   "source": [
    "# Test of first, basic model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff9429d-ec72-4e35-bfa5-e815befcd9ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.mixed_precision' has no attribute 'set_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmixed_precision\u001b[39;00m\n\u001b[1;32m     46\u001b[0m policy \u001b[38;5;241m=\u001b[39m mixed_precision\u001b[38;5;241m.\u001b[39mPolicy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixed_float16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_policy\u001b[49m(policy)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.mixed_precision' has no attribute 'set_policy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "# Set logging level to avoid unnecessary messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "# Set autograph verbosity to avoid unnecessary messages\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import keras\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_extra as tfe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "XC_ROOTDIR = '../../data/' # directory to save data in\n",
    "XC_DIR = 'test_dataset10' # subdirectory name of dataset\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9cad4-87aa-4b6f-871a-ebda0a2b1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069df00-2fba-496d-b715-1e8bf1ebe043",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6d1e2-6699-4983-9638-5d38b3395d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/dataset_train.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582e4e1-cb13-4714-93a9-4ade01a2debf",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce8f59-d368-48d4-88fb-ed0d0733e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    # random seed\n",
    "    seed = 42\n",
    "\n",
    "    # audio clip settings\n",
    "    sr = 22050\n",
    "    duration = 15 # the duration of the clips\n",
    "    \n",
    "    n_samples = duration*sr\n",
    "    \n",
    "    hop_length = 2048 # \"stepsize\" of the fft for the melspectrograms\n",
    "    nfft = 4096 # windowsize of the fft for the melspectrograms\n",
    "    n_mels = 128 # number of mel frequency bins\n",
    "    fmax = sr/2 # maximum frequency in the melspectrograms\n",
    "    input_dim = (n_mels, int(duration*sr//hop_length + 1))\n",
    "    \n",
    "    # training settings\n",
    "    batch_size = 32\n",
    "    n_epochs = 50\n",
    "    \n",
    "    # class labels/names\n",
    "    n_classes = len(np.unique(df.en))\n",
    "\n",
    "# set random seed in keras\n",
    "tf.keras.utils.set_random_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486464b-e522-4b07-9648-31d58c19cb4f",
   "metadata": {},
   "source": [
    "### Up- and downsample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2acf5-0cf2-4dcb-b561-3d45ec90158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_data(df, thr=200):\n",
    "    # get the class distribution\n",
    "    class_dist = df['en'].value_counts()\n",
    "\n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    down_classes = class_dist[class_dist < thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    up_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in down_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"en==@c\")\n",
    "        # find number of samples to add\n",
    "        num_up = thr - class_df.shape[0]\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=num_up, replace=True, random_state=cfg.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        up_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    up_df = pd.concat([df] + up_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return up_df\n",
    "\n",
    "def downsample_data(df, thr=400):\n",
    "    # get the class distribution\n",
    "    class_dist = df['en'].value_counts()\n",
    "    \n",
    "    # identify the classes that have less than the threshold number of samples\n",
    "    up_classes = class_dist[class_dist > thr].index.tolist()\n",
    "\n",
    "    # create an empty list to store the upsampled dataframes\n",
    "    down_dfs = []\n",
    "\n",
    "    # loop through the undersampled classes and upsample them\n",
    "    for c in up_classes:\n",
    "        # get the dataframe for the current class\n",
    "        class_df = df.query(\"en==@c\")\n",
    "        # Remove that class data\n",
    "        df = df.query(\"en!=@c\")\n",
    "        # upsample the dataframe\n",
    "        class_df = class_df.sample(n=thr, replace=False, random_state=cfg.seed)\n",
    "        # append the upsampled dataframe to the list\n",
    "        down_dfs.append(class_df)\n",
    "\n",
    "    # concatenate the upsampled dataframes and the original dataframe\n",
    "    down_df = pd.concat([df] + down_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return down_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f572a6-89a2-45e0-b941-df7e837dde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = upsample_data(downsample_data(df, thr=100), thr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d378ec-683e-4a70-b50a-accd4ea25262",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e088cf8-77f7-4857-959a-a80aae78955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fullfilename = \"../\" + df.fullfilename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29ee37-304f-4b0e-8e0e-aefedc7d38c9",
   "metadata": {},
   "source": [
    "### Functions, data generator etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1b30c-5e3d-4b96-9f9b-23839921df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates random integer # from https://www.kaggle.com/code/wengsilu/birdclef24pretraining\n",
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "# Generats random float\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd\n",
    "\n",
    "def pad_spectrogram(spec, shape = cfg.input_dim, random = False):\n",
    "    _ = np.zeros(shape)\n",
    "    if random:\n",
    "        rdm = random_int(maxval=shape[1]-spec.shape[1])\n",
    "        _[:,rdm: rdm + spec.shape[1]] = spec \n",
    "    else:\n",
    "        _[:,:spec.shape[1]] = spec\n",
    "    return _\n",
    "\n",
    "def load_spectrogram_slice(hdf5_path, name, start_row = 0, end_row =None, start_col = 0, end_col = None):\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        spectrogram_slice = f[name][start_row:end_row, start_col:end_col]\n",
    "    return spectrogram_slice\n",
    "\n",
    "def load_random_spec_slice(df, ID):\n",
    "    name = df.spectrogram.iloc[ID]\n",
    "    hdf5_path = os.path.dirname(df.fullfilename.iloc[ID]) + \"/spectrograms.h5\"\n",
    "    spec_length = df.length_spectrogram.iloc[ID]\n",
    "    if spec_length > cfg.input_dim[1]:\n",
    "        rdm = random_int(maxval= spec_length - cfg.input_dim[1])\n",
    "        return load_spectrogram_slice(hdf5_path = hdf5_path, name = name, start_col = rdm, end_col = rdm + cfg.input_dim[1])\n",
    "    elif spec_length < cfg.input_dim[1]:\n",
    "        return pad_spectrogram(load_spectrogram_slice(hdf5_path = hdf5_path, name = name), shape = cfg.input_dim, random = True)\n",
    "    return load_spectrogram_slice(hdf5_path = hdf5_path, name = name)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dataframe,\n",
    "                 batch_size=cfg.batch_size, \n",
    "                 dim=cfg.input_dim,\n",
    "                 n_channels =  1,\n",
    "                 n_classes=cfg.n_classes, \n",
    "                 shuffle=True\n",
    "                ):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dataframe = dataframe\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = load_random_spec_slice(self.dataframe, ID).reshape(*self.dim, self.n_channels)\n",
    "            # Store class\n",
    "            y[i] = self.dataframe.label.iloc[ID]\n",
    "        X = X.reshape(len(X), *self.dim, self.n_channels)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['accuracy'])\n",
    "    plt.plot(network_history.history['val_accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "def predict_file(df, ID, model):\n",
    "    name = df.spectrogram.iloc[ID]\n",
    "    hdf5_path = os.path.dirname(df.fullfilename.iloc[ID]) + \"/spectrograms.h5\"\n",
    "    spec_length = df.length_spectrogram.iloc[ID]\n",
    "    spec = load_spectrogram_slice(hdf5_path, name)\n",
    "    slices = []\n",
    "    \n",
    "    for i in range(spec_length//cfg.input_dim[1]):\n",
    "        slices.append(spec[:,i*cfg.input_dim[1]:(i+1)*cfg.input_dim[1]])\n",
    "    if spec_length%cfg.input_dim[1]/cfg.input_dim[1] > 5/cfg.duration:\n",
    "        # consider last slice, only if it is longer than the shortest clips in the dataset \n",
    "        slices.append(pad_spectrogram(spec[:, (i+1)*cfg.input_dim[1]:None], random = True))\n",
    "    \n",
    "    preds = model.predict(np.expand_dims(np.array(slices), axis = -1))\n",
    "    \n",
    "    return np.mean(preds, axis = 0) # return mean prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa05f3-cb66-4340-a2e6-48bb03c32839",
   "metadata": {},
   "source": [
    "## Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1c8db-8682-4b21-bb07-5f1f29539e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"CNN_Birdcall_classification\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"batch_size\": cfg.batch_size,\n",
    "        \"epochs\": cfg.n_epochs,\n",
    "        \"n_classes\": cfg.n_classes,\n",
    "        \"audio duration\": cfg.duration,\n",
    "        \"comment\": \"SGD, Selu\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb0c81-50a6-4c41-be67-80df89cf9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_metric = WandbMetricsLogger()\n",
    "\n",
    "#path = \"../models/Basemodel_test1.keras\"\n",
    "#\n",
    "#model_checkpoint = WandbModelCheckpoint(path, \n",
    "#                                        monitor = \"val_accuracy\", \n",
    "#                                        verbose = 1, \n",
    "#                                        save_best_only=True,\n",
    "#                                        mode = \"max\"\n",
    "#                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827c3d9-879a-4c00-b08e-7e3327a9d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "path = \"../models/Basemodel_test2.keras\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(path, \n",
    "                             monitor = \"val_accuracy\", \n",
    "                             verbose = 1, \n",
    "                             save_best_only=True,\n",
    "                             mode = \"max\"\n",
    "                            )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=6,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "\n",
    "log_file = \"../models/Basemodel_test2_log.csv\"\n",
    "\n",
    "csv_log = CSVLogger(log_file, separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64452a-b40a-48f3-9982-cfc062823b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [model_checkpoint, csv_log, WandbMetricsLogger()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a890af-b9ae-449c-aded-07cd71f96937",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85c771-e64c-4341-82d0-c66b02c87608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "\n",
    "tfm_layer = tfe.layers.TimeFreqMask(freq_mask_prob=0.5,\n",
    "                                  num_freq_masks=2,\n",
    "                                  freq_mask_param=15,\n",
    "                                  time_mask_prob=0.5,\n",
    "                                  num_time_masks=3,\n",
    "                                  time_mask_param=15,\n",
    "                                  time_last=True,\n",
    "                        )\n",
    "\n",
    "zscore_layer = tfe.layers.ZScoreMinMax()\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape=(*cfg.input_dim, 1))\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(inp)\n",
    "    x = tfm_layer(x)\n",
    "    \n",
    "    # Base model\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), padding='valid', activation='selu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='selu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(32, activation='selu')(x)\n",
    "    output = Dense(cfg.n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output, name = \"Basemodel\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# With batch normalization\n",
    "\n",
    "def build_model_batchNorm():\n",
    "    inp = Input(shape=(*cfg.input_dim, 1))\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(inp)\n",
    "    x = tfm_layer(x)\n",
    "    \n",
    "    # Base model\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=(3, 3), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    \n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    output = Dense(cfg.n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output, name = \"Batch Normalisation Model\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Model using DenseNet architecture\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "base_model = DenseNet121(include_top=False, input_shape=(*cfg.input_dim, 1), weights=None)\n",
    "\n",
    "def build_DenseNet():\n",
    "    inp = Input(shape=(*cfg.input_dim, 1))\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(inp)\n",
    "    # Time Frequency masking\n",
    "    #x = tfm_layer(x)\n",
    "    # Base model\n",
    "    x = base_model(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(cfg.n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output, name = \"DenseNet\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(include_top=False, input_shape=(*cfg.input_dim, 1), weights=None)\n",
    "\n",
    "def build_ResNet():\n",
    "    inp = Input(shape=(*cfg.input_dim, 1))\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(inp)\n",
    "    # Time Frequency masking\n",
    "    #x = tfm_layer(x)\n",
    "    # Base model\n",
    "    x = base_model(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    output = Dense(cfg.n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output, name = \"ResNet\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8300339-f4bf-4de7-b13b-651bcd2c0247",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e383534-3dcf-49d9-989f-3e7cbbc84048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "id_train, id_val, y_train, y_val = train_test_split(range(len(df)), df[\"label\"].to_list(), test_size = 0.3, random_state = cfg.seed)\n",
    "\n",
    "training_generator = DataGenerator(id_train, df)\n",
    "validation_generator = DataGenerator(id_val, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3627f1d-0fb3-41fc-b1f4-db5a6d145893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "#model = build_DenseNet()\n",
    "#model = build_ResNet()\n",
    "#model = build_model_batchNorm()\n",
    "\n",
    "hist = model.fit(training_generator,\n",
    "                 validation_data=validation_generator,\n",
    "                 verbose = 2, \n",
    "                 epochs = cfg.n_epochs,\n",
    "                 callbacks = callbacks\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ab5ea-6ec6-4e8d-aa5f-d3731096ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896ead2-b1d2-4452-80bd-3c0ce3b4b220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
