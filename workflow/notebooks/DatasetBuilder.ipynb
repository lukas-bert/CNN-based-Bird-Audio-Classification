{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb51ab7-bfe6-4ac9-bb88-e79da6724f15",
   "metadata": {},
   "source": [
    "# Build Dataset\n",
    "In this notebook the build dataset function, needed for the datapipeline is implemented\n",
    "\n",
    "This notebook is inspired by and partly copied from https://www.kaggle.com/code/wengsilu/birdclef24pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f906eb-9e34-4629-b167-dd9f881de0c3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afaf862-698e-4a57-8be5-70405efb5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 14:15:51.993002: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-28 14:15:51.993109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-28 14:15:51.994737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-28 14:15:52.005434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-28 14:15:54.095152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "# Set logging level to avoid unnecessary messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "# Set autograph verbosity to avoid unnecessary messages\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_extra as tfe\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36eb19-b2c3-4c8e-9d91-db6c575346a9",
   "metadata": {},
   "source": [
    "## Set device and strategy\n",
    "If GPUs are available, use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bb9e50-3854-4eb3-ad3c-3f4863a38aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on CPU\n"
     ]
    }
   ],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#gpus = tf.config.list_logical_devices('GPU')\n",
    "#ngpu = len(gpus) # Check number of GPUs\n",
    "#if ngpu:\n",
    "#    # Set GPU strategy\n",
    "#    strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
    "#    # Print GPU details\n",
    "#    print(\"> Running on GPU\", end=' | ')\n",
    "#    print(\"Num of GPUs: \", ngpu)\n",
    "#    device='GPU'\n",
    "#else:\n",
    "#    # If no GPUs are available, use CPU\n",
    "print(\"> Running on CPU\")\n",
    "strategy = tf.distribute.get_strategy()\n",
    "device='CPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5f147a-ae5c-42a1-8901-f1103d2c8a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# To find out which devices your operations and tensors are assigned to\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors and perform an operation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca1658-1c69-4840-a901-51befa49726f",
   "metadata": {},
   "source": [
    "## Load dataframe (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf89a5e-7741-4595-bd97-8d348211ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/dataset10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977d38d-bb78-4295-8d68-cffe4c613b7a",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76e8f20-32fd-4cbc-a136-e2099dd061e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    # random seed\n",
    "    seed = 42\n",
    "\n",
    "    # audio clip settings\n",
    "    sr = 24000\n",
    "    duration = 10\n",
    "    desired_length = duration*sr\n",
    "    \n",
    "    duration = 15 # the duration of the clips\n",
    "    n_samples = duration*sr\n",
    "    hop_length = 2048 # \"stepsize\" of the fft for the melspectrograms\n",
    "    nfft = 4096 # windowsize of the fft for the melspectrograms\n",
    "    n_mels = 128 # number of mel frequency bins\n",
    "    fmax = sr/2 # maximum frequency in the melspectrograms\n",
    "    input_dim = (int(duration*sr/hop_length + 1), n_mels)\n",
    "    \n",
    "    # data processing settings\n",
    "    batch_size = 16\n",
    "    shuffle_buffer = 256 # idk Number of elements from the dataset to buffer for shuffling.\n",
    "    \n",
    "    # class labels/names\n",
    "    names = list(np.unique(df.en))\n",
    "    num_classes = len(names)\n",
    "    labels = list(range(num_classes))\n",
    "    label2name = dict(zip(labels, names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "\n",
    "    # set device\n",
    "    device = device\n",
    "\n",
    "# set random seed in keras\n",
    "tf.keras.utils.set_random_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d8eb7-20a8-423c-a746-ada546b0f9cd",
   "metadata": {},
   "source": [
    "# Function to load and prepare audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6b3e5d-5021-4b96-8f2d-8dcd5d378b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates random integer # from https://www.kaggle.com/code/wengsilu/birdclef24pretraining\n",
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "# Generats random float\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f987fa-8236-4913-b584-60eafb3ff69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_loader(with_labels = True, cfg = cfg, num_classes = cfg.num_classes):\n",
    "    def decode(filepath):\n",
    "        # read audio\n",
    "        audio = tfio.audio.AudioIOTensor(filepath, dtype = tf.float32) # lazy load the file\n",
    "        rate = audio.rate\n",
    "        # cut out clip of specified duration at random position\n",
    "        num_samples = cfg.duration*rate\n",
    "        length = tf.cast(audio.shape[0], tf.int32)\n",
    "        if num_samples < length:\n",
    "            rdm = random_int(maxval = length - num_samples)\n",
    "            audio = audio[rdm:rdm+num_samples]\n",
    "        else:\n",
    "            audio = audio.to_tensor()\n",
    "        audio = tf.cast(audio, tf.float32)\n",
    "        # resample if necessary\n",
    "        audio = tfio.audio.resample(audio, tf.cast(rate, tf.int64), cfg.sr) if rate != cfg.sr else audio\n",
    "        # remove noise (tfio.audio.split() or tfio.audio.trim()?)# can't do this when the clip is already cut\n",
    "        # stereo to mono\n",
    "        audio = tf.reduce_mean(audio, axis=-1) if tf.shape(audio)[-1] == 2 else tf.squeeze(audio, axis = -1)\n",
    "        # pad if necessary\n",
    "        if tf.size(audio) < cfg.desired_length:\n",
    "            missing = cfg.desired_length - tf.size(audio)\n",
    "            rdm = random_int(maxval = missing)\n",
    "            audio = tf.pad(audio, [[rdm, missing-rdm]]) # pad rdm zeros left and missing-rdm zeros rigth\n",
    "        audio = tf.reshape(audio, [cfg.sr*cfg.duration])\n",
    "        return audio\n",
    "\n",
    "    def get_target(target):          \n",
    "        target = tf.reshape(target, [1])\n",
    "        target = tf.cast(tf.one_hot(target, num_classes), tf.float32) \n",
    "        target = tf.reshape(target, [num_classes])\n",
    "        return target\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        label = get_target(label)\n",
    "        return decode(path), label\n",
    "\n",
    "    return decode_with_labels if with_labels else decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d40ec-8c8e-4fcd-80fd-41b848d999e9",
   "metadata": {},
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d6a7964-8df2-4441-993d-6005fdad919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(paths, labels=None, batch_size=cfg.batch_size, target_size=[128, 256], # idk yet about target size\n",
    "                  audio_decode_fn=None,\n",
    "                  num_classes=cfg.num_classes,\n",
    "                  cache=True, cache_dir=\"\", drop_remainder=False,\n",
    "                  repeat=True, shuffle=cfg.shuffle_buffer):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from the given paths and labels.\n",
    "    \n",
    "    Args:\n",
    "        paths (list): A list of file paths to the audio files.\n",
    "        labels (list): A list of corresponding labels for the audio files.\n",
    "        batch_size (int): Batch size for the created dataset.\n",
    "        target_size (list): A list of target image size for the spectrograms.\n",
    "        audio_decode_fn (function): A function to decode the audio file.\n",
    "        cache (bool): Whether to cache the dataset or not.\n",
    "        cache_dir (str): Directory path to cache the dataset.\n",
    "        drop_remainder (bool): Whether to drop the last batch if it is smaller than batch_size.\n",
    "        repeat (bool): Whether to repeat the dataset or not.\n",
    "        shuffle (int): Number of elements from the dataset to buffer for shuffling.\n",
    "        \n",
    "    Returns:\n",
    "        ds (tf.data.Dataset): A TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    # Create cache directory if cache is enabled\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    # Set default audio decode function if not provided\n",
    "    if audio_decode_fn is None:\n",
    "        audio_decode_fn = audio_loader(with_labels = labels is not None, cfg = cfg, num_classes = cfg.num_classes)\n",
    "        \n",
    "    # Set TensorFlow AUTOTUNE option\n",
    "    AUTO = tf.data.experimental.AUTOTUNE # hopefully optimizes data pipeline and decreases loading times\n",
    "    # Create slices based on whether labels are provided\n",
    "    slices = (paths,) if labels is None else (paths, labels)\n",
    "    # Create TensorFlow dataset from slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    # Map audio decode function to dataset\n",
    "    ds = ds.map(audio_decode_fn, num_parallel_calls=AUTO)\n",
    "    # Cache dataset in memory if cache is enabled\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    # Repeat dataset indefinitely if repeat is enabled\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    # Create TensorFlow dataset options\n",
    "    opt = tf.data.Options()\n",
    "    # Shuffle dataset if shuffle is enabled\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=cfg.seed)\n",
    "        opt.experimental_deterministic = False\n",
    "    if cfg.device=='GPU':\n",
    "        # If the device is a GPU, turn off auto-sharding to avoid performance issues\n",
    "        opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    # Set the options for the dataset\n",
    "    ds = ds.with_options(opt)\n",
    "    # Batch the dataset with the specified batch size\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    # Prefetch the next batch of data to improve performance\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c81ce81-cffe-4d06-bf41-54bebf06305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 14:15:57.761009: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2024-06-28 14:15:57.766077: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.sample(frac=0.2, replace=False, random_state=cfg.seed)\n",
    "df_sample[\"fullfilename\"] = \"../\" + df_sample[\"fullfilename\"]\n",
    "paths = df_sample.fullfilename.tolist()\n",
    "names = df_sample.en.tolist()\n",
    "labels = []\n",
    "for name in names:\n",
    "    labels.append(cfg.name2label[name])\n",
    "\n",
    "ds = build_dataset(paths, labels, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d6b3c9-19ed-42c2-830e-04fc8c24797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(batch, row=3, col=3, label2name=None,):\n",
    "    \"\"\"Plot one batch data\"\"\"\n",
    "    if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "        audios, tars = batch\n",
    "    else:\n",
    "        audios = batch\n",
    "        tars = None\n",
    "    plt.figure(figsize=(col*5, row*3))\n",
    "    for idx in range(row*col):\n",
    "        ax = plt.subplot(row, col, idx+1)\n",
    "        plt.plot(audios[idx].numpy(), color=cmap(0.1))\n",
    "        if tars is not None:\n",
    "            label = tars[idx].numpy().argmax()\n",
    "            name = label2name[label]\n",
    "            plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b648820-e411-4bce-8c12-134fa717c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audios, labels = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f846135-c75c-4d7c-930b-82f0bbb98880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_batch((audios, labels), label2name=cfg.label2name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edd9521-800e-4e1d-85d2-4371b63d696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "melspec_layer = tfe.layers.MelSpectrogram(n_fft=cfg.nfft, \n",
    "                                          hop_length=cfg.hop_length, \n",
    "                                          sr=cfg.sr, \n",
    "                                          fmin=0,\n",
    "                                          fmax=cfg.fmax,\n",
    "                                         )\n",
    "\n",
    "zscore_layer = tfe.layers.ZScoreMinMax()\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape=(cfg.n_samples,))\n",
    "    \n",
    "    # Spectrogram\n",
    "    x = melspec_layer(inp)\n",
    "    \n",
    "    # Normalize\n",
    "    x = zscore_layer(x)\n",
    "    \n",
    "    # Add a channel dimension\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    \n",
    "    # Base model\n",
    "    x = Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), padding='valid', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding=\"valid\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(cfg.num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d9f166-8fd3-42e0-9970-698b4a4c51c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 360000)]          0         \n",
      "                                                                 \n",
      " mel_spectrogram (MelSpectr  (None, 128, 176)          0         \n",
      " ogram)                                                          \n",
      "                                                                 \n",
      " z_score_min_max (ZScoreMin  (None, 128, 176)          0         \n",
      " Max)                                                            \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda  (None, 128, 176, 1)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 126, 174, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 87, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 85, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 42, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 42, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80640)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5161024   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5182250 (19.77 MB)\n",
      "Trainable params: 5182250 (19.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4185af2d-81f8-407b-a07c-ac5c58c489b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fullfilename\"] = \"../\" + df[\"fullfilename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe8191c-25a3-43cf-87eb-f4c1397dacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "id_train, id_val, y_train, y_val = train_test_split(range(len(df)), df[\"en\"].to_list(), test_size = 0.3, random_state = cfg.seed)\n",
    "\n",
    "#paths, labels=None, batch_size=cfg.batch_size, target_size=[128, 256], # idk yet about target size\n",
    "#                  audio_decode_fn=None,\n",
    "#                  num_classes=cfg.num_classes,\n",
    "#                  cache=True, cache_dir=\"\", drop_remainder=False,\n",
    "#                  repeat=True, shuffle=cfg.shuffle_buffer):\n",
    "\n",
    "paths_train = list(df.iloc[id_train].fullfilename)\n",
    "paths_val = list(df.iloc[id_val].fullfilename)\n",
    "\n",
    "label_train = []\n",
    "for y in y_train:\n",
    "    label_train.append(cfg.name2label[y])\n",
    "label_val = []\n",
    "for y in y_val:\n",
    "    label_val.append(cfg.name2label[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7bba49c-cfde-46b2-b003-986fe56860d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_dataset(paths_train, label_train)\n",
    "valid_ds = build_dataset(paths_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e0c61e1-ca22-4d63-be4a-4e3db2e5d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node Reshape_2 defined at (most recent call last):\n<stack traces unavailable>\nInput to reshape is a tensor with 240000 values, but the requested shape has 360000\n\t [[{{node Reshape_2}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_2865]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/conda/envs/ML_Birds/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/conda/envs/ML_Birds/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node Reshape_2 defined at (most recent call last):\n<stack traces unavailable>\nInput to reshape is a tensor with 240000 values, but the requested shape has 360000\n\t [[{{node Reshape_2}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_2865]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=2, \n",
    "        validation_data=valid_ds,\n",
    "        steps_per_epoch=len(paths_train)/cfg.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d2901-1fcd-411f-b6ab-a51af6b9e183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
